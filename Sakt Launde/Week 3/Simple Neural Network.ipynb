{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Implementing Neural Network From Scratch"
      ],
      "metadata": {
        "id": "uEC77ZPUrGVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we import the required libraries."
      ],
      "metadata": {
        "id": "WzlJlgvnrO_Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "WRze9m_V2wBU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define our Activation Function. Activation Function is used to add non-linearity in our model and normalise the output of our model. In this case, we choose the Sigmoid Function. This function returns values between 0 and 1."
      ],
      "metadata": {
        "id": "NAhIQqZ_rUas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "_6M61K7l234a"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the derivative of the derivative of the Sigmoid Function. This will be used later on in Back Propogation."
      ],
      "metadata": {
        "id": "rSwUovw0sGw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_deriv(x):\n",
        "  return (np.exp(-x)/(1 + np.exp(-x))**2)"
      ],
      "metadata": {
        "id": "swYd26gQ9jcg"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to define our neural network we create a class NeuralNetwork. Our neural network takes 3 inputs, passes it through 2 hidden layers of 5 and 4 neurons and then these neurons fire into an output layer of 3 nodes. "
      ],
      "metadata": {
        "id": "cDL2GJ2gsee6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each neuron corresponds to a particular feature in our dataset. We then assign different weights to the features marking their importance. We then meausre our error by comparing the output with desired output. Now we keep tweaking the weights for a number of iterations until we minimise our error and start getting the desired result."
      ],
      "metadata": {
        "id": "qSGooJWB_sTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Forward Propogation, input to one layer is multiplied by corresponding weights and is passes on to next layer. This proccess is carried out starting from our input layer to the last hidden layer. Then the result is passed onto the output layer. "
      ],
      "metadata": {
        "id": "4siJtn2PTTet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(object):\n",
        "  def __init__(self,Inputs=3,HiddenLayers=[5,4],Outputs=2):\n",
        "    self.Inputs=Inputs\n",
        "    self.HiddenLayers=HiddenLayers\n",
        "    self.Outputs=Outputs\n",
        "\n",
        "    layers=[self.Inputs]+self.HiddenLayers+[self.Outputs]\n",
        "# We implement random weights to train the model. Weights are multiplied by inputs at each layer. Weights are a measure of importance of a neuron in getting the desired output from our model. \n",
        "    weights = []\n",
        "    for i in range(len(layers)-1):\n",
        "      w=np.random.rand(layers[i], layers[i+1])\n",
        "      weights.append(w)\n",
        "    self.weights=weights\n",
        "\n",
        "# We create an empty array for activations. Activation are the activation function applied on result of multiplication of inputs and weights\n",
        "    activations =[]\n",
        "    for i in range(len(layers)):\n",
        "      a=np.zeros(layers[i])\n",
        "      activations.append(a)\n",
        "    self.activations=activations\n",
        "\n",
        "# We create an empty array for derivatives to calculate the gradient\n",
        "    derivatives=[]\n",
        "    for i in range(len(layers)-1):\n",
        "      a=np.zeros((layers[i],layers[i+1]))\n",
        "      derivatives.append(a)\n",
        "    self.derivatives=derivatives\n",
        "\n",
        "# We create a function forward to iterate over layers, apply weight and sigmoid function,get activations and get output after passing through all the hidden layers\n",
        "  def forward(self,Inputs):\n",
        "    activation=Inputs\n",
        "    self.activations[0]=Inputs\n",
        "    for i, w in enumerate(self.weights):\n",
        "      unactivatedInputs=np.dot(activation,w)\n",
        "      activation=sigmoid(unactivatedInputs)\n",
        "      self.activations[i+1]=activation\n",
        "    # print(self.activations)\n",
        "    return activation\n",
        "# We created a loss function which is a measure of how much the output given by the model deviates from the desired output. \n",
        "  def loss(self,Output,RightOutput):\n",
        "    return (RightOutput-Output)**2\n",
        "# We created a function to calculate the derivative of loss function.\n",
        "  def loss_deriv(self,Output,RightOutput):\n",
        "    return (2*(RightOutput-Output))\n",
        "# Gradient Descent is an algorithm through which we increase efficiency of training of a neural network on a dataset. In this algorithm, aim is to minimize the \"loss function\".To do this, we calculate the gradient and move in the direction\n",
        "# of negative gradient of the loss function in each iteration to get to the minimum. The weights are adjusted accordingly at each iteration.\n",
        "# So for each iteration we have,\n",
        "# New weights= weights+(learning rate x Gradient)\n",
        "  def GradientDescent(self,LearningRate):\n",
        "    deriv_lr=[]\n",
        "    for d in self.derivatives:\n",
        "      for i in range(len(d)):\n",
        "        d[i]=LearningRate*d[i]\n",
        "      deriv_lr.append(d)\n",
        "    New_weights=self.weights+deriv_lr\n",
        "    self.weights=New_weights\n",
        "    return New_weights\n",
        "# We create Back function for back propogation. Through back propogation we calculate gradient of the loss function i.e dL/dW.\n",
        "# Now we use chain rule of derivative,\n",
        "# dL/d(W for current layer)=dL/d(activationsfor current layer) x d(activations for current layer)/d(unactivated inputs for current layer) x d(unactivated inputs for current layer)/d(W for previous layer)\n",
        "  def Back(self,loss_deriv):\n",
        "    for i in reversed(range(len(self.derivatives))):\n",
        "        Sigmoid_Deriv=sigmoid_deriv(self.activations[i+1])\n",
        "        delta=loss_deriv*Sigmoid_Deriv\n",
        "        delta_reshaped=delta.reshape(delta.shape[0],-1).T\n",
        "        activation=self.activations[i]\n",
        "        activation=activation.reshape(activation.shape[0],-1)\n",
        "\n",
        "        self.derivatives[i]=np.dot(activation,delta_reshaped)\n",
        "\n",
        "        loss_deriv=np.dot(delta,self.weights[i].T)\n",
        "        \n",
        "    return loss_deriv\n",
        "# We create the Train function giving instructions on how to train the model\n",
        "  def Train(self,Inputs,RightOutput,epochs,LearningRate):\n",
        "    losses=[]\n",
        "    for i in range(epochs):\n",
        "      Output=self.forward(Inputs[i])\n",
        "\n",
        "      Loss=self.loss(Output,RightOutput[i])\n",
        "      losses.append(Loss)\n",
        "      Loss_Deriv=self.loss_deriv(Output,RightOutput[i])\n",
        "\n",
        "      self.Back(Loss_Deriv)\n",
        "\n",
        "      self.GradientDescent(LearningRate)\n",
        "\n",
        "      print(\"epoch:{}    loss:{}\".format(i+1,Loss))\n",
        "    return losses"
      ],
      "metadata": {
        "id": "6WvBgthul2Fb"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now create a random dataset and train our model on it"
      ],
      "metadata": {
        "id": "0MaX8g47AoIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ ==\"__main__\":\n",
        "  NN=NeuralNetwork()\n",
        "  # inputs=[]\n",
        "  # Inputs=np.random.rand(NN.Inputs)\n",
        "  # Inputs=Inputs.reshape()\n",
        "  \n",
        "  # Creating a random dataset and its correct outputs to train our model \n",
        "  dataset=np.random.rand(NN.Inputs,1000) #for i in range(0,NN.Inputs) for j in range(0,1000)])\n",
        "  dataset=dataset.reshape([1000,3])\n",
        "  rightOutput=np.random.choice([0,1],size=1000)\n",
        "# Calling the Train() function to train our dataset\n",
        "  NN.Train(dataset,rightOutput,100,0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "XAIazsRFxz_1",
        "outputId": "740a7cee-465f-42a9-e516-fddfdd9ab0c5"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1    loss:[0.75595361 0.62072168]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-6dc6f1e224b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mrightOutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Calling the Train() function to train our dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrightOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-164-520986cb53dd>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, Inputs, RightOutput, epochs, LearningRate)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0mOutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m       \u001b[0mLoss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRightOutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-164-520986cb53dd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Inputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0munactivatedInputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m       \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munactivatedInputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (3,5) not aligned: 2 (dim 0) != 3 (dim 0)"
          ]
        }
      ]
    }
  ]
}